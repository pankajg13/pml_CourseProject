---
title: "Coursera PML Course Project"
author: "Pankaj Goyal"
date: "Saturday, July 26, 2014"
output: html_document
---

### Objective
The data was collected by Velloso, Bulling, Gellersen, Ugulino, and Fuks  and their results presented in "Qualitative Activity Recognition of Weight Lifting Exercises."; Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) ; Stuttgart, Germany: ACM SIGCHI, 2013. 

The goal of this project is to predict the manner in which they did the exercise. They placed sensors on the arm, forearm, belt of the subjects and on dumbbell used.

### Analysis

The data provided to us was already splitted into a training and a test set. Training data is used to explore and develop a predictive model and used the final model on the test data to evaluate the results.

```{r load libraries,warning=FALSE,results='hold'}
# load the libraries
library(caret)
library(rattle)
library(gridExtra)
library(randomForest)
```

```{r load data,warning=FALSE}
# read the data
# training  <-  read.csv("pml-training.csv", header = T, strip.white = T, stringsAsFactors = F)
# testing   <-  read.csv("pml-testing.csv",  header = T, strip.white = T, stringsAsFactors = F)
training  <-  read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), header = T, strip.white = T, stringsAsFactors = F)
testing   <-  read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),  header = T, strip.white = T, stringsAsFactors = F)
```

The data was imported as character in many cases and that is incorrect, and for now colums 6-159 should be numeric

```{r, warning=FALSE}
# dataset used: training
train     <-  training
charVar   <-  which(sapply(train,is.character))
train[charVar[4:36]]  <-  lapply(train[charVar[4:36]],as.numeric)
```

There is a lot of missing data, most varibles are either complete or missing, chose the ones with full data

```{r Data cleaning, warning = FALSE}
noNA      <-  which(sapply(train,function(x) sum(is.na(x)))==0)
noNA      <-  noNA[-(1:7)]
train_noNA<-  subset(train,,noNA)
```

I performed Principal Components Analysis, in order to reduce the number of features, dimensions and noise, on the set of measures for each sensor and then combined such measures in a new training set

```{r PCA, warning = FALSE, results = 'hold'}
exercise  <-  c("_belt","_arm","_dumbbell","_forearm")

# create PC's for belt sensor
pc_belt   <-  preProcess(train_noNA[which(grepl(exercise[1], names(train_noNA)))], method = "pca")
pc_belt_new         <-  predict(pc_belt, train_noNA[which(grepl(exercise[1], names(train_noNA)))])
names(pc_belt_new)  <-  tolower(paste(names(pc_belt_new),exercise[1], sep = ""))

# create PC's for arm sensor
pc_arm    <-  preProcess(train_noNA[which(grepl(exercise[2], names(train_noNA)))], method = "pca")
pc_arm_new          <-  predict(pc_arm, train_noNA[which(grepl(exercise[2], names(train_noNA)))])
names(pc_arm_new)   <-  tolower(paste(names(pc_arm_new), exercise[2], sep = ""))

# create PC's for dumbbell sensor
pc_dumbbell <-  preProcess(train_noNA[which(grepl(exercise[3], names(train_noNA)))],method = "pca")
pc_dumbbell_new       <-  predict(pc_dumbbell, train_noNA[which(grepl(exercise[3], names(train_noNA)))])
names(pc_dumbbell_new)<-  tolower(paste(names(pc_dumbbell_new), exercise[3], sep = ""))

# create PC's for forearm sensor
pc_forearm  <-  preProcess(train_noNA[which(grepl(exercise[4], names(train_noNA)))], method = "pca")
pc_forearm_new        <-  predict(pc_forearm, train_noNA[which(grepl(exercise[4], names(train_noNA)))])
names(pc_forearm_new) <-  tolower(paste(names(pc_forearm_new), exercise[4], sep = ""))
```

Visual exploratory analysis of the data was performed by looking at the distributions by exercise class for each variable. 

```{r Plot PCAs, warning = FALSE}
# Visual exploratory analysis
p1  <-  ggplot(pc_belt_new, aes(x = pc1_belt, y = pc2_belt, color = train_noNA$classe)) + geom_point()
p2  <-  ggplot(pc_arm_new, aes(x = pc1_arm, y = pc2_arm, color = train_noNA$classe)) + geom_point()
p3  <-  ggplot(pc_dumbbell_new , aes(x = pc1_dumbbell, y = pc2_dumbbell, color = train_noNA$classe)) + geom_point() + coord_cartesian(ylim=c(0,10))
p4  <-  ggplot(pc_forearm_new, aes(x = pc1_forearm, y = pc2_forearm, color = train_noNA$classe)) + geom_point() + coord_cartesian(ylim=c(-5,5))
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

The charts seem to group but not perfectly, From such analysis it is clear that some variables shows very good separation at least among some classes.

```{r Combine Training PCA, warning = FALSE, results = 'hold'}
# Combining all the PCs into one training PC Analysis set
training_pc   <-  data.frame(cbind(pc_belt_new, pc_arm_new, pc_dumbbell_new, pc_forearm_new), classe = train_noNA$classe)
```

Applying the PC'S to test dataset;
```{r Combine Testing PCA, warning = FALSE, results = 'hold'}
test          <-  testing

# for belt sensor
test_pc_belt  <-  predict(pc_belt, newdata = test[row.names(pc_belt$rotation)])
names(test_pc_belt)   <-  tolower(paste(names(test_pc_belt), "_belt", sep = ""))

# for arm sensor
test_pc_arm   <-  predict(pc_arm, newdata = test[row.names(pc_arm$rotation)])
names(test_pc_arm)    <-  tolower(paste(names(test_pc_arm), "_arm", sep = ""))

# for dumbbell sensor
test_pc_dumbbell  <- predict(pc_dumbbell, newdata = test[row.names(pc_dumbbell$rotation)])
names(test_pc_dumbbell)<- tolower(paste(names(test_pc_dumbbell), "_dumbbell", sep = ""))

# for forearm sensor
test_pc_forearm   <- predict(pc_forearm, newdata = test[row.names(pc_forearm$rotation)])
names(test_pc_forearm)<-  tolower(paste(names(test_pc_forearm), "_forearm", sep = ""))

# Combining all the PCs into one testing PC Analysis set
testing_pc        <-  data.frame(cbind(test_pc_belt, test_pc_arm, test_pc_dumbbell, test_pc_forearm))
```

#### Prediction Models
- Model 1: Method (rpart): Due to its ease of interpretation, an attempt to model using a single CART tree using the caret package has made.

```{r CART Tree, warning = FALSE}
set.seed(78786)
# MOdel 1: (rpart method)
model1  <-  train(classe ~ ., data = training_pc, method = "rpart")

# print the final model
model1$finalModel

# fancy rpart plotiing of the final model
fancyRpartPlot(model1$finalModel)

# Confusion Matrix
confusionMatrix(predict(model1,training_pc),training_pc$classe)
```

But Unfortunately this was not successful as the resulting model did not predict 2 of the classes (C and E) and had very low accuracy of 38.7% and kappa of 18.5%.

- MOdel 2: Random Forest Method:

```{r Random Forest, warning = FALSE}
set.seed(46776)
# Model2: Random Forest Method
model2      <-  randomForest(classe ~ ., data = training_pc)

# Confusion Matrix
confusionMatrix(predict(model2, training_pc), training_pc$classe)

# Plot
results_rf  <-  data.frame(actual = training_pc$classe, predicted = predict(model2, training_pc))
ggplot(results_rf, aes(x = actual, y = predicted, color = actual)) + geom_jitter(size = 0.5, alpha = 0.5) + theme_bw() + theme(legend.position = "none")
```

A we can see from this Fancy plot that the Random Forest Model performed very well with reported accuracy of 100% and Kappa of 100%. These measures were estimated for the out of sample using 25 bootstrap samples.

Applying the model to test set,
```{r Predict, warning = FALSE, results = 'hold'}
#predict and generate the cases for evaluation
testing_predict   <-  predict(model2, testing_pc)

# change the working directory
setwd("/Users/Pankaj/pml_CourseProject/predictions/")
source("/Users/Pankaj/pml_CourseProject/pml_write_files.R")
pml_write_files(testing_predict)
```

The model is applied to the testing dataset (after transforming it with the same principal component used for the train set), We can see that this model was able to correctly predict all 20 cases.

The interpretation of the meaning of principal components is always challenging. In this case it is more so given that the measurements of the sensors deal with concepts not very familiar to most people (yaw, roll, pitch).